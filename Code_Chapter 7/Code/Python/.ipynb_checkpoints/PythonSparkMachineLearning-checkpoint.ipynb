{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Regression Use case==============\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Regression Use case==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - Change this directory to the right location where the data is stored\n",
    "dataDir = \"/Users/RajT/Downloads/wine-quality/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the the RDD by reading the wine data from the disk \n",
    "lines = sc.textFile(dataDir + \"winequality-red.csv\")\n",
    "splitLines = lines.map(lambda l: l.split(\";\"))\n",
    "# Vector is a data type with 0 based indices and double-typed values. In that there are two types namely dense and sparse.\n",
    "# A dense vector is backed by a double array representing its entry values, \n",
    "# A sparse vector is backed by two parallel arrays: indices and values\n",
    "wineDataRDD = splitLines.map(lambda p: (float(p[11]), Vectors.dense([float(p[0]), float(p[1]), float(p[2]), float(p[3]), float(p[4]), float(p[5]), float(p[6]), float(p[7]), float(p[8]), float(p[9]), float(p[10])])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  5.0|[7.4,0.7,0.0,1.9,...|\n",
      "|  5.0|[7.8,0.88,0.0,2.6...|\n",
      "|  5.0|[7.8,0.76,0.04,2....|\n",
      "|  6.0|[11.2,0.28,0.56,1...|\n",
      "|  5.0|[7.4,0.7,0.0,1.9,...|\n",
      "|  5.0|[7.4,0.66,0.0,1.8...|\n",
      "|  5.0|[7.9,0.6,0.06,1.6...|\n",
      "|  7.0|[7.3,0.65,0.0,1.2...|\n",
      "|  7.0|[7.8,0.58,0.02,2....|\n",
      "|  5.0|[7.5,0.5,0.36,6.1...|\n",
      "|  5.0|[6.7,0.58,0.08,1....|\n",
      "|  5.0|[7.5,0.5,0.36,6.1...|\n",
      "|  5.0|[5.6,0.615,0.0,1....|\n",
      "|  5.0|[7.8,0.61,0.29,1....|\n",
      "|  5.0|[8.9,0.62,0.18,3....|\n",
      "|  5.0|[8.9,0.62,0.19,3....|\n",
      "|  7.0|[8.5,0.28,0.56,1....|\n",
      "|  5.0|[8.1,0.56,0.28,1....|\n",
      "|  4.0|[7.4,0.59,0.08,4....|\n",
      "|  6.0|[7.9,0.32,0.51,1....|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the data frame containing the training data having two columns. 1) The actula output or label of the data 2) The vector containing the features\n",
    "trainingDF = spark.createDataFrame(wineDataRDD, ['label', 'features'])\n",
    "trainingDF.show()\n",
    "# Create the object of the algorithm which is the Linear Regression with the parameters\n",
    "# Linear regression parameter to make lr.fit() use at most 10 iterations\n",
    "lr = LinearRegression(maxIter=10)\n",
    "# Create a trained model by fitting the parameters using the training data\n",
    "model = lr.fit(trainingDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  5.0|[7.4,0.7,0.0,1.9,...|\n",
      "|  5.0|[7.8,0.88,0.0,2.6...|\n",
      "|  7.0|[7.3,0.65,0.0,1.2...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Once the model is prepared, to test the model, prepare the test data containing the labels and feature vectors\n",
    "testDF = spark.createDataFrame([\n",
    "    (5.0, Vectors.dense([7.4, 0.7, 0.0, 1.9, 0.076, 25.0, 67.0, 0.9968, 3.2, 0.68,9.8])),\n",
    "    (5.0, Vectors.dense([7.8, 0.88, 0.0, 2.6, 0.098, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4])),\n",
    "    (7.0, Vectors.dense([7.3, 0.65, 0.0, 1.2, 0.065, 15.0, 18.0, 0.9968, 3.36, 0.57, 9.5]))], [\"label\", \"features\"])\n",
    "testDF.createOrReplaceTempView(\"test\")\n",
    "testDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------------+\n",
      "|            features|label|       prediction|\n",
      "+--------------------+-----+-----------------+\n",
      "|[7.4,0.7,0.0,1.9,...|  5.0|5.352730835898477|\n",
      "|[7.8,0.88,0.0,2.6...|  5.0|4.817999362011964|\n",
      "|[7.3,0.65,0.0,1.2...|  7.0|5.280106355653388|\n",
      "+--------------------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the transformation of the test data using the model and predict the output values or lables. This is to compare the predicted value and the actual label value\n",
    "testTransform = model.transform(testDF)\n",
    "tested = testTransform.select(\"features\", \"label\", \"prediction\")\n",
    "tested.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[7.4,0.7,0.0,1.9,...|\n",
      "|[7.8,0.88,0.0,2.6...|\n",
      "|[7.3,0.65,0.0,1.2...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare a data set without the output/lables to predict the output using the trained model\n",
    "predictDF = spark.sql(\"SELECT features FROM test\")\n",
    "predictDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|            features|       prediction|\n",
      "+--------------------+-----------------+\n",
      "|[7.4,0.7,0.0,1.9,...|5.352730835898477|\n",
      "|[7.8,0.88,0.0,2.6...|4.817999362011964|\n",
      "|[7.3,0.65,0.0,1.2...|5.280106355653388|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the transformation with the predict data set and display the predictions\n",
    "predictTransform = model.transform(predictDF)\n",
    "predicted = predictTransform.select(\"features\", \"prediction\")\n",
    "predicted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Classification Use case==============\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Classification Use case==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - Change this directory to the right location where the data is stored\n",
    "dataDir = \"/Users/RajT/Downloads/wine-quality/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the the RDD by reading the wine data from the disk \n",
    "lines = sc.textFile(dataDir + \"winequality-white.csv\")\n",
    "splitLines = lines.map(lambda l: l.split(\";\"))\n",
    "wineDataRDD = splitLines.map(lambda p: (float(0) if (float(p[11]) < 7) else float(1), Vectors.dense([float(p[0]), float(p[1]), float(p[2]), float(p[3]), float(p[4]), float(p[5]), float(p[6]), float(p[7]), float(p[8]), float(p[9]), float(p[10])])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|[7.0,0.27,0.36,20...|\n",
      "|  0.0|[6.3,0.3,0.34,1.6...|\n",
      "|  0.0|[8.1,0.28,0.4,6.9...|\n",
      "|  0.0|[7.2,0.23,0.32,8....|\n",
      "|  0.0|[7.2,0.23,0.32,8....|\n",
      "|  0.0|[8.1,0.28,0.4,6.9...|\n",
      "|  0.0|[6.2,0.32,0.16,7....|\n",
      "|  0.0|[7.0,0.27,0.36,20...|\n",
      "|  0.0|[6.3,0.3,0.34,1.6...|\n",
      "|  0.0|[8.1,0.22,0.43,1....|\n",
      "|  0.0|[8.1,0.27,0.41,1....|\n",
      "|  0.0|[8.6,0.23,0.4,4.2...|\n",
      "|  0.0|[7.9,0.18,0.37,1....|\n",
      "|  1.0|[6.6,0.16,0.4,1.5...|\n",
      "|  0.0|[8.3,0.42,0.62,19...|\n",
      "|  1.0|[6.6,0.17,0.38,1....|\n",
      "|  0.0|[6.3,0.48,0.04,1....|\n",
      "|  1.0|[6.2,0.66,0.48,1....|\n",
      "|  0.0|[7.4,0.34,0.42,1....|\n",
      "|  0.0|[6.5,0.31,0.14,7....|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the data frame containing the training data having two columns. 1) The actula output or label of the data 2) The vector containing the features\n",
    "trainingDF = spark.createDataFrame(wineDataRDD, ['label', 'features'])\n",
    "# Create the object of the algorithm which is the Logistic Regression with the parameters\n",
    "# LogisticRegression parameter to make lr.fit() use at most 10 iterations and the regularization parameter.\n",
    "# When a higher degree polynomial used by the algorithm to fit a set of points in a linear regression model, to prevent overfitting, regularization is used and this parameter is just for that\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# Create a trained model by fitting the parameters using the training data\n",
    "model = lr.fit(trainingDF)\n",
    "trainingDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|[6.1,0.32,0.24,1....|\n",
      "|  0.0|[5.2,0.44,0.04,1....|\n",
      "|  0.0|[7.2,0.32,0.47,5....|\n",
      "|  0.0|[6.4,0.595,0.14,5...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Once the model is prepared, to test the model, prepare the test data containing the labels and feature vectors\n",
    "testDF = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([6.1,0.32,0.24,1.5,0.036,43,140,0.9894,3.36,0.64,10.7])),\n",
    "    (0.0, Vectors.dense([5.2,0.44,0.04,1.4,0.036,38,124,0.9898,3.29,0.42,12.4])),\n",
    "    (0.0, Vectors.dense([7.2,0.32,0.47,5.1,0.044,19,65,0.9951,3.38,0.36,9])),    \n",
    "    (0.0, Vectors.dense([6.4,0.595,0.14,5.2,0.058,15,97,0.991,3.03,0.41,12.6]))], [\"label\", \"features\"])\n",
    "testDF.createOrReplaceTempView(\"test\")\n",
    "testDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|[6.1,0.32,0.24,1....|  1.0|       0.0|\n",
      "|[5.2,0.44,0.04,1....|  0.0|       0.0|\n",
      "|[7.2,0.32,0.47,5....|  0.0|       0.0|\n",
      "|[6.4,0.595,0.14,5...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the transformation of the test data using the model and predict the output values or lables. This is to compare the predicted value and the actual label value\n",
    "testTransform = model.transform(testDF)\n",
    "tested = testTransform.select(\"features\", \"label\", \"prediction\")\n",
    "tested.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[6.1,0.32,0.24,1....|       0.0|\n",
      "|[5.2,0.44,0.04,1....|       0.0|\n",
      "|[7.2,0.32,0.47,5....|       0.0|\n",
      "|[6.4,0.595,0.14,5...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare a data set without the output/lables to predict the output using the trained model\n",
    "predictDF = spark.sql(\"SELECT features FROM test\")\n",
    "# Do the transformation with the predict data set and display the predictions\n",
    "predictTransform = model.transform(predictDF)\n",
    "predicted = testTransform.select(\"features\", \"prediction\")\n",
    "predicted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Spam Filtering Use case==============\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Spam Filtering Use case==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare training documents from a list of messages from emails used to filter them as spam or not spam\n",
    "# If the original message is a spam then the label is 1 and if the message is genuine then the label is 0\n",
    "LabeledDocument = Row(\"email\", \"message\", \"label\")\n",
    "training = spark.createDataFrame([\n",
    "    (\"you@example.com\", \"hope you are well\", 0.0),\n",
    "    (\"raj@example.com\", \"nice to hear from you\", 0.0),\n",
    "    (\"thomas@example.com\", \"happy holidays\", 0.0),\n",
    "    (\"mark@example.com\", \"see you tomorrow\", 0.0),\n",
    "    (\"xyz@example.com\", \"save money\", 1.0),  \n",
    "    (\"top10@example.com\", \"low interest rate\", 1.0),\n",
    "    (\"marketing@example.com\", \"cheap loan\", 1.0)], [\"email\", \"message\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|               email|             message|label|\n",
      "+--------------------+--------------------+-----+\n",
      "|     you@example.com|   hope you are well|  0.0|\n",
      "|     raj@example.com|nice to hear from...|  0.0|\n",
      "|  thomas@example.com|      happy holidays|  0.0|\n",
      "|    mark@example.com|    see you tomorrow|  0.0|\n",
      "|     xyz@example.com|          save money|  1.0|\n",
      "|   top10@example.com|   low interest rate|  1.0|\n",
      "|marketing@example...|          cheap loan|  1.0|\n",
      "+--------------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure an Spark machin learning pipeline, consisting of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"message\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "# LogisticRegression parameter to make lr.fit() use at most 10 iterations and the regularization parameter.\n",
    "# When a higher degree polynomial used by the algorithm to fit a set of points in a linear regression model, to prevent overfitting, regularization is used and this parameter is just for that\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "# Fit the pipeline to train the model to study the messages\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+\n",
      "|            email|        message|\n",
      "+-----------------+---------------+\n",
      "|  you@example.com|    how are you|\n",
      "| jain@example.com|hope doing well|\n",
      "|caren@example.com|want some money|\n",
      "| zhou@example.com|    secure loan|\n",
      "|  ted@example.com|      need loan|\n",
      "+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare messages for prediction, which are not categorized and leaving upto the algorithm to predict\n",
    "test = spark.createDataFrame([\n",
    "    (\"you@example.com\", \"how are you\"),\n",
    "    (\"jain@example.com\", \"hope doing well\"),\n",
    "    (\"caren@example.com\", \"want some money\"),\n",
    "    (\"zhou@example.com\", \"secure loan\"),\n",
    "    (\"ted@example.com\",\"need loan\")], [\"email\", \"message\"])\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+----------+\n",
      "|            email|        message|prediction|\n",
      "+-----------------+---------------+----------+\n",
      "|  you@example.com|    how are you|       0.0|\n",
      "| jain@example.com|hope doing well|       0.0|\n",
      "|caren@example.com|want some money|       1.0|\n",
      "| zhou@example.com|    secure loan|       1.0|\n",
      "|  ted@example.com|      need loan|       1.0|\n",
      "+-----------------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the new messages\n",
    "prediction = model.transform(test).select(\"email\", \"message\", \"prediction\")\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Finding Synonyms==============\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Finding Synonyms==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - Change this directory to the right location where the data is stored\n",
    "dataDir = \"/Users/RajT/Downloads/20_newsgroups/*\"\n",
    "# Read the entire text into a DataFrame\n",
    "textRDD = sc.wholeTextFiles(dataDir).map(lambda recs: Row(sentence=recs[1]))\n",
    "textDF = spark.createDataFrame(textRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the sentences to words\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", gaps=False, pattern=\"\\\\w+\")\n",
    "tokenizedDF = regexTokenizer.transform(textDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the Estimator\n",
    "# It sets the vector size, and the parameter minCount sets the minimum number of times a token must appear to be included in the word2vec model's vocabulary.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"result\")\n",
    "# Train the model\n",
    "model = word2Vec.fit(tokenizedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     word|        similarity|\n",
      "+---------+------------------+\n",
      "| strapped|0.9999918504219028|\n",
      "|    bingo|0.9999909957939888|\n",
      "|collected|0.9999907658056393|\n",
      "|  kingdom|0.9999896797527402|\n",
      "| presumed|0.9999806586578037|\n",
      "| patients|0.9999778970248504|\n",
      "|    azats|0.9999718388241235|\n",
      "|  opening| 0.999969723774294|\n",
      "|  holdout|0.9999685636131942|\n",
      "| contrast|0.9999677676714386|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find 10 synonyms of a given word\n",
    "synonyms1 = model.findSynonyms(\"gun\", 10)\n",
    "synonyms1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|       word|        similarity|\n",
      "+-----------+------------------+\n",
      "|   peaceful|0.9999983523475047|\n",
      "|  democracy|0.9999964568156694|\n",
      "|      areas| 0.999994036518118|\n",
      "|  miniscule|0.9999920828755365|\n",
      "|       lame|0.9999877327660102|\n",
      "|    strikes|0.9999877253180771|\n",
      "|terminology|0.9999839393584438|\n",
      "|      wrath|0.9999829348358952|\n",
      "|    divided| 0.999982619125983|\n",
      "|    hillary|0.9999795817857984|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find 10 synonyms of a different word\n",
    "synonyms2 = model.findSynonyms(\"crime\", 10)\n",
    "synonyms2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
