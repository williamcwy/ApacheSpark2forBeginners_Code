{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "# Creation of the list from where the RDD is going to be created\n",
    "acTransList = [\"SB10001,1000\", \"SB10002,1200\", \"SB10003,8000\", \"SB10004,400\", \"SB10005,300\", \"SB10006,10000\", \"SB10007,500\", \"SB10008,56\", \"SB10009,30\",\"SB10010,7000\", \"CR10001,7000\", \"SB10002,-10\"]\n",
    "# Create the DataFrame\n",
    "acTransDF = sc.parallelize(acTransList).map(lambda trans: trans.split(\",\")).map(lambda p: Row(accNo=p[0], tranAmount=float(p[1]))).toDF()\n",
    "# Register temporary table in the DataFrame for using it in SQL\n",
    "acTransDF.createOrReplaceTempView(\"trans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- accNo: string (nullable = true)\n",
      " |-- tranAmount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the structure of the DataFrame\n",
    "acTransDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10001|    1000.0|\n",
      "|SB10002|    1200.0|\n",
      "|SB10003|    8000.0|\n",
      "|SB10004|     400.0|\n",
      "|SB10005|     300.0|\n",
      "|SB10006|   10000.0|\n",
      "|SB10007|     500.0|\n",
      "|SB10008|      56.0|\n",
      "|SB10009|      30.0|\n",
      "|SB10010|    7000.0|\n",
      "|CR10001|    7000.0|\n",
      "|SB10002|     -10.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few records of the DataFrame\n",
    "acTransDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10001|    1000.0|\n",
      "|SB10002|    1200.0|\n",
      "|SB10003|    8000.0|\n",
      "|SB10004|     400.0|\n",
      "|SB10005|     300.0|\n",
      "|SB10006|   10000.0|\n",
      "|SB10007|     500.0|\n",
      "|SB10008|      56.0|\n",
      "|SB10009|      30.0|\n",
      "|SB10010|    7000.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use SQL to create another DataFrame containing the good transaction records\n",
    "goodTransRecords = spark.sql(\"SELECT accNo, tranAmount FROM trans WHERE accNo like 'SB%' AND tranAmount > 0\")\n",
    "# Register temporary table in the DataFrame for using it in SQL\n",
    "goodTransRecords.createOrReplaceTempView(\"goodtrans\")\n",
    "# Show the first few records of the DataFrame\n",
    "goodTransRecords.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10002|    1200.0|\n",
      "|SB10003|    8000.0|\n",
      "|SB10006|   10000.0|\n",
      "|SB10010|    7000.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use SQL to create another DataFrame containing the high value transaction records\n",
    "highValueTransRecords = spark.sql(\"SELECT accNo, tranAmount FROM goodtrans WHERE tranAmount > 1000\")\n",
    "# Show the first few records of the DataFrame\n",
    "highValueTransRecords.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|CR10001|    7000.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use SQL to create another DataFrame containing the bad account records\n",
    "badAccountRecords = spark.sql(\"SELECT accNo, tranAmount FROM trans WHERE accNo NOT like 'SB%'\")\n",
    "# Show the first few records of the DataFrame\n",
    "badAccountRecords.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10002|     -10.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use SQL to create another DataFrame containing the bad amount records\n",
    "badAmountRecords = spark.sql(\"SELECT accNo, tranAmount FROM trans WHERE tranAmount < 0\")\n",
    "# Show the first few records of the DataFrame\n",
    "badAmountRecords.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|CR10001|    7000.0|\n",
      "|SB10002|     -10.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the union of two DataFrames and create another DataFrame\n",
    "badTransRecords = badAccountRecords.union(badAmountRecords)\n",
    "# Show the first few records of the DataFrame\n",
    "badTransRecords.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    sum|\n",
      "+-------+\n",
      "|28486.0|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sum\n",
    "sumAmount = spark.sql(\"SELECT sum(tranAmount)as sum FROM goodtrans\")\n",
    "# Show the first few records of the DataFrame\n",
    "sumAmount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    max|\n",
      "+-------+\n",
      "|10000.0|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the maximum\n",
    "maxAmount = spark.sql(\"SELECT max(tranAmount) as max FROM goodtrans\")\n",
    "# Show the first few records of the DataFrame\n",
    "maxAmount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| min|\n",
      "+----+\n",
      "|30.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the minimum\n",
    "minAmount = spark.sql(\"SELECT min(tranAmount)as min FROM goodtrans\")\n",
    "# Show the first few records of the DataFrame\n",
    "minAmount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  accNo|\n",
      "+-------+\n",
      "|SB10001|\n",
      "|SB10002|\n",
      "|SB10003|\n",
      "|SB10004|\n",
      "|SB10005|\n",
      "|SB10006|\n",
      "|SB10007|\n",
      "|SB10008|\n",
      "|SB10009|\n",
      "|SB10010|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use SQL to create another DataFrame containing the good account numbers\n",
    "goodAccNos = spark.sql(\"SELECT DISTINCT accNo FROM trans WHERE accNo like 'SB%' ORDER BY accNo\")\n",
    "# Show the first few records of the DataFrame\n",
    "goodAccNos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28486.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the sum using mixing of DataFrame and RDD like operations\n",
    "sumAmountByMixing = goodTransRecords.rdd.map(lambda trans: trans.tranAmount).reduce(lambda a,b : a+b)\n",
    "sumAmountByMixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the maximum using mixing of DataFrame and RDD like operations\n",
    "maxAmountByMixing = goodTransRecords.rdd.map(lambda trans: trans.tranAmount).reduce(lambda a,b : a if a > b else b)\n",
    "maxAmountByMixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the minimum using mixing of DataFrame and RDD like operations\n",
    "minAmountByMixing = goodTransRecords.rdd.map(lambda trans: trans.tranAmount).reduce(lambda a,b : a if a < b else b)\n",
    "minAmountByMixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10001|    1000.0|\n",
      "|SB10002|    1200.0|\n",
      "|SB10003|    8000.0|\n",
      "|SB10004|     400.0|\n",
      "|SB10005|     300.0|\n",
      "|SB10006|   10000.0|\n",
      "|SB10007|     500.0|\n",
      "|SB10008|      56.0|\n",
      "|SB10009|      30.0|\n",
      "|SB10010|    7000.0|\n",
      "|CR10001|    7000.0|\n",
      "|SB10002|     -10.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few records of the DataFrame\n",
    "acTransDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- accNo: string (nullable = true)\n",
      " |-- tranAmount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the structure of the DataFrame\n",
    "acTransDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10001|    1000.0|\n",
      "|SB10002|    1200.0|\n",
      "|SB10003|    8000.0|\n",
      "|SB10004|     400.0|\n",
      "|SB10005|     300.0|\n",
      "|SB10006|   10000.0|\n",
      "|SB10007|     500.0|\n",
      "|SB10008|      56.0|\n",
      "|SB10009|      30.0|\n",
      "|SB10010|    7000.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame using API for the good transaction records\n",
    "goodTransRecords = acTransDF.filter(\"accNo like 'SB%'\").filter(\"tranAmount > 0\")\n",
    "# Show the first few records of the DataFrame\n",
    "goodTransRecords.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10002|    1200.0|\n",
      "|SB10003|    8000.0|\n",
      "|SB10006|   10000.0|\n",
      "|SB10010|    7000.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame using API for the high value transaction records\n",
    "highValueTransRecords = goodTransRecords.filter(\"tranAmount > 1000\")\n",
    "# Show the first few records of the DataFrame\n",
    "highValueTransRecords.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|CR10001|    7000.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame using API for the bad account records\n",
    "badAccountRecords = acTransDF.filter(\"accNo NOT like 'SB%'\")\n",
    "# Show the first few records of the DataFrame\n",
    "badAccountRecords.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10002|     -10.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame using API for the bad amount records\n",
    "badAmountRecords = acTransDF.filter(\"tranAmount < 0\")\n",
    "# Show the first few records of the DataFrame\n",
    "badAmountRecords.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|CR10001|    7000.0|\n",
      "|SB10002|     -10.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the union of two DataFrames and create another DataFrame\n",
    "badTransRecords = badAccountRecords.union(badAmountRecords)\n",
    "# Show the first few records of the DataFrame\n",
    "badTransRecords.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|sum(tranAmount)|\n",
      "+---------------+\n",
      "|        28486.0|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sum\n",
    "sumAmount = goodTransRecords.agg({\"tranAmount\": \"sum\"})\n",
    "# Show the first few records of the DataFrame\n",
    "sumAmount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|max(tranAmount)|\n",
      "+---------------+\n",
      "|        10000.0|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the maximum\n",
    "maxAmount = goodTransRecords.agg({\"tranAmount\": \"max\"})\n",
    "# Show the first few records of the DataFrame\n",
    "maxAmount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|min(tranAmount)|\n",
      "+---------------+\n",
      "|           30.0|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the minimum\n",
    "minAmount = goodTransRecords.agg({\"tranAmount\": \"min\"})\n",
    "# Show the first few records of the DataFrame\n",
    "minAmount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  accNo|\n",
      "+-------+\n",
      "|SB10001|\n",
      "|SB10002|\n",
      "|SB10003|\n",
      "|SB10004|\n",
      "|SB10005|\n",
      "|SB10006|\n",
      "|SB10007|\n",
      "|SB10008|\n",
      "|SB10009|\n",
      "|SB10010|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame using API for the good account numbers\n",
    "goodAccNos = acTransDF.filter(\"accNo like 'SB%'\").select(\"accNo\").distinct().orderBy(\"accNo\")\n",
    "# Show the first few records of the DataFrame\n",
    "goodAccNos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Persist the data of the DataFrame into a Parquet file\n",
    "acTransDF.write.parquet(\"python.trans.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data into a DataFrame from the Parquet file\n",
    "acTransDFfromParquet = spark.read.parquet(\"python.trans.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10002|    1200.0|\n",
      "|SB10003|    8000.0|\n",
      "|SB10005|     300.0|\n",
      "|SB10006|   10000.0|\n",
      "|SB10008|      56.0|\n",
      "|SB10009|      30.0|\n",
      "|CR10001|    7000.0|\n",
      "|SB10002|     -10.0|\n",
      "|SB10001|    1000.0|\n",
      "|SB10004|     400.0|\n",
      "|SB10007|     500.0|\n",
      "|SB10010|    7000.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few records of the DataFrame\n",
    "acTransDFfromParquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|transTotal|\n",
      "+-------+----------+\n",
      "|SB10005|      56.0|\n",
      "|SB10004|     500.0|\n",
      "|SB10003|     330.0|\n",
      "|SB10002|    8590.0|\n",
      "|SB10001|   18900.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creation of the list from where the RDD is going to be created\n",
    "acTransList = [\"SB10001,1000\", \"SB10002,1200\", \"SB10001,8000\",\"SB10002,400\", \"SB10003,300\", \"SB10001,10000\",\"SB10004,500\",\"SB10005,56\",\"SB10003,30\",\"SB10002,7000\", \"SB10001,-100\",\"SB10002,-10\"]\n",
    "# Create the DataFrame\n",
    "acTransDF = sc.parallelize(acTransList).map(lambda trans: trans.split(\",\")).map(lambda p: Row(accNo=p[0], tranAmount=float(p[1]))).toDF()\n",
    "# Register temporary table in the DataFrame for using it in SQL\n",
    "acTransDF.createOrReplaceTempView(\"trans\")\n",
    "# Use SQL to create another DataFrame containing the account summary records\n",
    "acSummary = spark.sql(\"SELECT accNo, sum(tranAmount) as transTotal FROM trans GROUP BY accNo\")\n",
    "# Show the first few records of the DataFrame\n",
    "acSummary.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|transTotal|\n",
      "+-------+----------+\n",
      "|SB10005|      56.0|\n",
      "|SB10004|     500.0|\n",
      "|SB10003|     330.0|\n",
      "|SB10002|    8590.0|\n",
      "|SB10001|   18900.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame using API for the account summary records\n",
    "acSummaryViaDFAPI = acTransDF.groupBy(\"accNo\").agg({\"tranAmount\": \"sum\"}).selectExpr(\"accNo\", \"`sum(tranAmount)` as transTotal\")\n",
    "# Show the first few records of the DataFrame\n",
    "acSummaryViaDFAPI.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+\n",
      "|  accNo|firstName|lastName|\n",
      "+-------+---------+--------+\n",
      "|SB10001|    Roger| Federer|\n",
      "|SB10002|     Pete| Sampras|\n",
      "|SB10003|   Rafael|   Nadal|\n",
      "|SB10004|    Boris|  Becker|\n",
      "|SB10005|     Ivan|   Lendl|\n",
      "+-------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creation of the list from where the RDD is going to be created\n",
    "AcMaster = Row('accNo', 'firstName', 'lastName')\n",
    "AcBal = Row('accNo', 'balanceAmount')\n",
    "acMasterList = [\"SB10001,Roger,Federer\",\"SB10002,Pete,Sampras\", \"SB10003,Rafael,Nadal\",\"SB10004,Boris,Becker\", \"SB10005,Ivan,Lendl\"]\n",
    "acBalList = [\"SB10001,50000\", \"SB10002,12000\",\"SB10003,3000\", \"SB10004,8500\", \"SB10005,5000\"]\n",
    "# Create the DataFrame\n",
    "acMasterDF = sc.parallelize(acMasterList).map(lambda trans: trans.split(\",\")).map(lambda r: AcMaster(*r)).toDF()\n",
    "acBalDF = sc.parallelize(acBalList).map(lambda trans: trans.split(\",\")).map(lambda r: AcBal(r[0], float(r[1]))).toDF()\n",
    "# Persist the data of the DataFrame into a Parquet file\n",
    "acMasterDF.write.parquet(\"python.master.parquet\")\n",
    "# Persist the data of the DataFrame into a JSON file\n",
    "acBalDF.write.json(\"pythonMaster.json\")\n",
    "# Read the data into a DataFrame from the Parquet file\n",
    "acMasterDFFromFile = spark.read.parquet(\"python.master.parquet\")\n",
    "# Register temporary table in the DataFrame for using it in SQL\n",
    "acMasterDFFromFile.createOrReplaceTempView(\"master\")\n",
    "# Register temporary table in the DataFrame for using it in SQL\n",
    "acBalDFFromFile = spark.read.json(\"pythonMaster.json\")\n",
    "# Register temporary table in the DataFrame for using it in SQL\n",
    "acBalDFFromFile.createOrReplaceTempView(\"balance\")\n",
    "# Show the first few records of the DataFrame\n",
    "acMasterDFFromFile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|  accNo|balanceAmount|\n",
      "+-------+-------------+\n",
      "|SB10001|      50000.0|\n",
      "|SB10002|      12000.0|\n",
      "|SB10003|       3000.0|\n",
      "|SB10004|       8500.0|\n",
      "|SB10005|       5000.0|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few records of the DataFrame\n",
    "acBalDFFromFile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+-------------+\n",
      "|  accNo|firstName|lastName|balanceAmount|\n",
      "+-------+---------+--------+-------------+\n",
      "|SB10001|    Roger| Federer|      50000.0|\n",
      "|SB10002|     Pete| Sampras|      12000.0|\n",
      "|SB10004|    Boris|  Becker|       8500.0|\n",
      "|SB10005|     Ivan|   Lendl|       5000.0|\n",
      "|SB10003|   Rafael|   Nadal|       3000.0|\n",
      "+-------+---------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use SQL to create another DataFrame containing the account detail records\n",
    "acDetail = spark.sql(\"SELECT master.accNo, firstName, lastName, balanceAmount FROM master, balance WHERE master.accNo = balance.accNo ORDER BY balanceAmount DESC\")\n",
    "# Show the first few records of the DataFrame\n",
    "acDetail.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+-------------+\n",
      "|  accNo|firstName|lastName|balanceAmount|\n",
      "+-------+---------+--------+-------------+\n",
      "|SB10001|    Roger| Federer|      50000.0|\n",
      "|SB10002|     Pete| Sampras|      12000.0|\n",
      "|SB10004|    Boris|  Becker|       8500.0|\n",
      "|SB10005|     Ivan|   Lendl|       5000.0|\n",
      "|SB10003|   Rafael|   Nadal|       3000.0|\n",
      "+-------+---------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame using API for the account detail records\n",
    "acDetailFromAPI = acMasterDFFromFile.join(acBalDFFromFile, acMasterDFFromFile.accNo == acBalDFFromFile.accNo).sort(acBalDFFromFile.balanceAmount, ascending=False).select(acMasterDFFromFile.accNo, acMasterDFFromFile.firstName, acMasterDFFromFile.lastName, acBalDFFromFile.balanceAmount)\n",
    "# Show the first few records of the DataFrame\n",
    "acDetailFromAPI.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+-------------+\n",
      "|  accNo|firstName|lastName|balanceAmount|\n",
      "+-------+---------+--------+-------------+\n",
      "|SB10001|    Roger| Federer|      50000.0|\n",
      "|SB10002|     Pete| Sampras|      12000.0|\n",
      "|SB10004|    Boris|  Becker|       8500.0|\n",
      "+-------+---------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use SQL to create another DataFrame containing the top 3 account detail records\n",
    "acDetailTop3 = spark.sql(\"SELECT master.accNo, firstName, lastName, balanceAmount FROM master, balance WHERE master.accNo = balance.accNo ORDER BY balanceAmount DESC\").limit(3)\n",
    "# Show the first few records of the DataFrame\n",
    "acDetailTop3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
